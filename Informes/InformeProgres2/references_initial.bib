@inproceedings{xia_diffir_2023,
	title = {{DiffIR}: {Efficient} {Diffusion} {Model} for {Image} {Restoration}},
	shorttitle = {{DiffIR}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-11-07},
	author = {Xia, Bin and Zhang, Yulun and Wang, Shiyin and Wang, Yitong and Wu, Xinglong and Tian, Yapeng and Yang, Wenming and Van Gool, Luc},
	year = {2023},
	pages = {13095--13105},
	file = {Full Text PDF:C\:\\Users\\polri\\Zotero\\storage\\B929BMWR\\Xia et al. - 2023 - DiffIR Efficient Diffusion Model for Image Restoration.pdf:application/pdf},
}


@misc{restormer,
      title={Restormer: Efficient Transformer for High-Resolution Image Restoration}, 
      author={Syed Waqas Zamir and Aditya Arora and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Ming-Hsuan Yang},
      year={2022},
      eprint={2111.09881},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.09881}, 
}
@article{ivanova23analogue,
	title        = {Simulating analogue film damage to analyse and improve artefact restoration on high-resolution scans},
	author       = {Daniela Ivanova and John Williamson and Paul Henderson},
	year         = 2023,
	journal      = {Computer Graphics Forum (Proc. Eurographics 2023)},
	volume = {42},
	number = {2},   
	doi = {https://doi.org/10.1111/cgf.14749},
	copyright = {Creative Commons Attribution 4.0 International}
}


@misc{kaji_overview_2019,
	title = {Overview of image-to-image translation by use of deep neural networks: denoising, super-resolution, modality conversion, and reconstruction in medical imaging},
	shorttitle = {Overview of image-to-image translation by use of deep neural networks},
	url = {http://arxiv.org/abs/1905.08603},
	doi = {10.48550/arXiv.1905.08603},
	abstract = {Since the advent of deep convolutional neural networks (DNNs), computer vision has seen an extremely rapid progress that has led to huge advances in medical imaging. This article does not aim to cover all aspects of the field but focuses on a particular topic, image-to-image translation. Although the topic may not sound familiar, it turns out that many seemingly irrelevant applications can be understood as instances of image-to-image translation. Such applications include (1) noise reduction, (2) super-resolution, (3) image synthesis, and (4) reconstruction. The same underlying principles and algorithms work for various tasks. Our aim is to introduce some of the key ideas on this topic from a uniform point of view. We introduce core ideas and jargon that are specific to image processing by use of DNNs. Having an intuitive grasp of the core ideas of and a knowledge of technical terms would be of great help to the reader for understanding the existing and future applications. Most of the recent applications which build on image-to-image translation are based on one of two fundamental architectures, called pix2pix and CycleGAN, depending on whether the available training data are paired or unpaired. We provide computer codes which implement these two architectures with various enhancements. Our codes are available online with use of the very permissive MIT license. We provide a hands-on tutorial for training a model for denoising based on our codes. We hope that this article, together with the codes, will provide both an overview and the details of the key algorithms, and that it will serve as a basis for the development of new applications.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Kaji, Shizuo and Kida, Satoshi},
	month = jun,
	year = {2019},
	note = {arXiv:1905.08603},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\I4NRWMXZ\\Kaji and Kida - 2019 - Overview of image-to-image translation by use of deep neural networks denoising, super-resolution,.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\H4KNFTM2\\1905.html:text/html},
}


@inproceedings{liang_swinir_2021,
	address = {Montreal, BC, Canada},
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-0191-3},
	shorttitle = {{SwinIR}},
	url = {https://ieeexplore.ieee.org/document/9607618/},
	doi = {10.1109/ICCVW54120.2021.00210},
	abstract = {Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from lowquality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67\%.},
	language = {en},
	urldate = {2024-11-07},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	publisher = {IEEE},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	month = oct,
	year = {2021},
	pages = {1833--1844},
	file = {PDF:C\:\\Users\\polri\\Zotero\\storage\\R8945HCQ\\Liang et al. - 2021 - SwinIR Image Restoration Using Swin Transformer.pdf:application/pdf},
}
@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}
@misc{kaji_overview_2019,
	title = {Overview of image-to-image translation by use of deep neural networks: denoising, super-resolution, modality conversion, and reconstruction in medical imaging},
	shorttitle = {Overview of image-to-image translation by use of deep neural networks},
	url = {http://arxiv.org/abs/1905.08603},
	doi = {10.48550/arXiv.1905.08603},
	abstract = {Since the advent of deep convolutional neural networks (DNNs), computer vision has seen an extremely rapid progress that has led to huge advances in medical imaging. This article does not aim to cover all aspects of the field but focuses on a particular topic, image-to-image translation. Although the topic may not sound familiar, it turns out that many seemingly irrelevant applications can be understood as instances of image-to-image translation. Such applications include (1) noise reduction, (2) super-resolution, (3) image synthesis, and (4) reconstruction. The same underlying principles and algorithms work for various tasks. Our aim is to introduce some of the key ideas on this topic from a uniform point of view. We introduce core ideas and jargon that are specific to image processing by use of DNNs. Having an intuitive grasp of the core ideas of and a knowledge of technical terms would be of great help to the reader for understanding the existing and future applications. Most of the recent applications which build on image-to-image translation are based on one of two fundamental architectures, called pix2pix and CycleGAN, depending on whether the available training data are paired or unpaired. We provide computer codes which implement these two architectures with various enhancements. Our codes are available online with use of the very permissive MIT license. We provide a hands-on tutorial for training a model for denoising based on our codes. We hope that this article, together with the codes, will provide both an overview and the details of the key algorithms, and that it will serve as a basis for the development of new applications.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Kaji, Shizuo and Kida, Satoshi},
	month = jun,
	year = {2019},
	note = {arXiv:1905.08603},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\I4NRWMXZ\\Kaji and Kida - 2019 - Overview of image-to-image translation by use of deep neural networks denoising, super-resolution,.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\H4KNFTM2\\1905.html:text/html},
}

@inproceedings{xia_diffir_2023,
	title = {{DiffIR}: {Efficient} {Diffusion} {Model} for {Image} {Restoration}},
	shorttitle = {{DiffIR}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-11-07},
	author = {Xia, Bin and Zhang, Yulun and Wang, Shiyin and Wang, Yitong and Wu, Xinglong and Tian, Yapeng and Yang, Wenming and Van Gool, Luc},
	year = {2023},
	pages = {13095--13105},
	file = {Full Text PDF:C\:\\Users\\polri\\Zotero\\storage\\B929BMWR\\Xia et al. - 2023 - DiffIR Efficient Diffusion Model for Image Restoration.pdf:application/pdf},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\56YJCJIL\\Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\EBDT6TKR\\2105.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\TSJUKC5A\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\8YAJAUUZ\\2006.html:text/html},
}

@misc{xie_segformer_2021,
	title = {{SegFormer}: {Simple} and {Efficient} {Design} for {Semantic} {Segmentation} with {Transformers}},
	shorttitle = {{SegFormer}},
	url = {http://arxiv.org/abs/2105.15203},
	doi = {10.48550/arXiv.2105.15203},
	abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	month = oct,
	year = {2021},
	note = {arXiv:2105.15203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\RAKNXSCB\\Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semantic Segmentation with Transformers.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\6N7HL2YZ\\2105.html:text/html},
}

@misc{tam_stable_2024,
	title = {Stable {Diffusion} {Project}: {Reviving} {Old} {Photos}},
	shorttitle = {Stable {Diffusion} {Project}},
	url = {https://www.machinelearningmastery.com/stable-diffusion-project-reviving-old-photos/},
	abstract = {Photography has been around for more than a century. There are many old photos around, and probably your family has some, too. Limited by the camera and film of the time, you may have photos of low resolution, blurry, or with folds or scratches. Restoring these old photos and making them like new ones taken […]},
	language = {en-US},
	urldate = {2025-02-04},
	journal = {MachineLearningMastery.com},
	author = {Tam, Adrian},
	month = jun,
	year = {2024},
	file = {Snapshot:C\:\\Users\\polri\\Zotero\\storage\\GGVKVLEK\\stable-diffusion-project-reviving-old-photos.html:text/html},
}

@misc{xie_star_2025,
	title = {{STAR}: {Spatial}-{Temporal} {Augmentation} with {Text}-to-{Video} {Models} for {Real}-{World} {Video} {Super}-{Resolution}},
	shorttitle = {{STAR}},
	url = {http://arxiv.org/abs/2501.02976},
	doi = {10.48550/arXiv.2501.02976},
	abstract = {Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models ({\textbackslash}textit\{e.g.\}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce{\textbackslash}textbf\{{\textasciitilde}{\textbackslash}name\} ({\textbackslash}textbf\{S\}patial-{\textbackslash}textbf\{T\}emporal {\textbackslash}textbf\{A\}ugmentation with T2V models for {\textbackslash}textbf\{R\}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate{\textbackslash}textbf\{{\textasciitilde}{\textbackslash}name\}{\textasciitilde}outperforms state-of-the-art methods on both synthetic and real-world datasets.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Xie, Rui and Liu, Yinhong and Zhou, Penghao and Zhao, Chen and Zhou, Jun and Zhang, Kai and Zhang, Zhenyu and Yang, Jian and Yang, Zhenheng and Tai, Ying},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02976 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\4BUJWIGL\\Xie et al. - 2025 - STAR Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\MIS3TCGC\\2501.html:text/html},
}

@misc{noauthor_nju-pcalabstar_2025,
	title = {{NJU}-{PCALab}/{STAR}},
	url = {https://github.com/NJU-PCALab/STAR},
	abstract = {STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution},
	urldate = {2025-02-10},
	publisher = {NJU-PCALab},
	month = feb,
	year = {2025},
	note = {original-date: 2024-11-21T10:14:56Z},
}

@misc{zhu_denoising_2023,
	title = {Denoising {Diffusion} {Models} for {Plug}-and-{Play} {Image} {Restoration}},
	url = {http://arxiv.org/abs/2305.08995},
	doi = {10.48550/arXiv.2305.08995},
	abstract = {Plug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at \{{\textbackslash}url\{https://github.com/yuanzhi-zhu/DiffPIR\}\}},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Zhu, Yuanzhi and Zhang, Kai and Liang, Jingyun and Cao, Jiezhang and Wen, Bihan and Timofte, Radu and Gool, Luc Van},
	month = may,
	year = {2023},
	note = {arXiv:2305.08995 [cs]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\MSFVNZ99\\Zhu et al. - 2023 - Denoising Diffusion Models for Plug-and-Play Image Restoration.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\XNNTDQ9B\\2305.html:text/html},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
	file = {Full Text PDF:C\:\\Users\\polri\\Zotero\\storage\\VP2IWZPM\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf},
}

@misc{alom_recurrent_2018,
	title = {Recurrent {Residual} {Convolutional} {Neural} {Network} based on {U}-{Net} ({R2U}-{Net}) for {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.06955},
	doi = {10.48550/arXiv.1802.06955},
	abstract = {Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including U-Net and residual U-Net (ResU-Net).},
	urldate = {2025-02-26},
	publisher = {arXiv},
	author = {Alom, Md Zahangir and Hasan, Mahmudul and Yakopcic, Chris and Taha, Tarek M. and Asari, Vijayan K.},
	month = may,
	year = {2018},
	note = {arXiv:1802.06955 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 12 pages, 21 figures, 3 Tables},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\VZ223PZD\\Alom et al. - 2018 - Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentat.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\XB54TWID\\1802.html:text/html},
}

@misc{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	shorttitle = {Attention {U}-{Net}},
	url = {http://arxiv.org/abs/1804.03999},
	doi = {10.48550/arXiv.1804.03999},
	abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
	urldate = {2025-02-26},
	publisher = {arXiv},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	month = may,
	year = {2018},
	note = {arXiv:1804.03999 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to published in MIDL'18 (Revised Version) / OpenReview link: https://openreview.net/forum?id=Skft7cijM},
	file = {Preprint PDF:C\:\\Users\\polri\\Zotero\\storage\\FVCQTKD8\\Oktay et al. - 2018 - Attention U-Net Learning Where to Look for the Pancreas.pdf:application/pdf;Snapshot:C\:\\Users\\polri\\Zotero\\storage\\BSKIIVRT\\1804.html:text/html},
}

@misc{leejunhyun_leejunhyunimage_segmentation_2025,
	title = {{LeeJunHyun}/{Image}\_Segmentation},
	url = {https://github.com/LeeJunHyun/Image_Segmentation},
	abstract = {Pytorch implementation of U-Net, R2U-Net, Attention U-Net, and Attention R2U-Net.},
	urldate = {2025-02-26},
	author = {leejunhyun},
	month = feb,
	year = {2025},
	note = {original-date: 2018-06-18T08:27:27Z},
}

@misc{ivanova_daniela997filmdamagesimulator_2025,
	title = {daniela997/{FilmDamageSimulator}},
	copyright = {MIT},
	url = {https://github.com/daniela997/FilmDamageSimulator},
	abstract = {Scans of empty 35mm film frames with annotated artifacts such as dust, scratches, dirt, hairs.},
	urldate = {2025-03-10},
	author = {Ivanova, Daniela},
	month = feb,
	year = {2025},
	note = {original-date: 2022-08-09T16:23:48Z},
}

@misc{newsreel, url={https://newsreels.net/}, journal={Hearst Metrotone News Collection}} 
@INPROCEEDINGS{9857234,
	
	author={Brempong, Emmanuel Asiedu and Kornblith, Simon and Chen, Ting and Parmar, Niki and Minderer, Matthias and Norouzi, Mohammad},
	
	booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
	
	title={Denoising Pretraining for Semantic Segmentation}, 
	
	year={2022},
	
	volume={},
	
	number={},
	
	pages={4174-4185},
	
	keywords={Training;Image segmentation;Noise reduction;Semantics;Supervised learning;Transformers;Probabilistic logic},
	
	doi={10.1109/CVPRW56347.2022.00462}}

@misc{lugmayr2022repaintinpaintingusingdenoising,
	title={RePaint: Inpainting using Denoising Diffusion Probabilistic Models}, 
	author={Andreas Lugmayr and Martin Danelljan and Andres Romero and Fisher Yu and Radu Timofte and Luc Van Gool},
	year={2022},
	eprint={2201.09865},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2201.09865}, 
}